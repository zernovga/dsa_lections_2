---
theme: sirius-college
exportFilename: final/dsa_lection_9
mdc: true
layout: cover
---

# Основы алгоритмизации и программирования<br>Лекция 9. Поиск подстроки в строке. Хеширование

Наивный алгоритм. Z-функция, префикс функция, алгоритм Кнута-Морриса-Пратта. Полиномиальный хеш, алгоритм Рабина-Карпа. Сортировка строк хешами. Хеш-таблицы. Коллизии. Открытая и закрытая адресация. Гипотеза равномерного хеширования.

---

# Поиск подстроки в строке

**Поиск подстроки в строке** — одна из простейших задач поиска информации. Применяется в виде встроенной функции в текстовых редакторах, СУБД, поисковых машинах, языках программирования и т. п.

**Поиск подстроки в строке (String searching algorithm)** — класс алгоритмов над строками, которые позволяют найти паттерн (pattern) в тексте (text).

**Формулировка задачи:** Дан текст $t[0..n−1]$ и паттерн $p[0..m−1]$ такие, что $n⩾m$ и элементы этих строк — символы из конечного алфавита $Σ$. Требуется проверить, входит ли паттерн $p$ в текст $t$.

---

# Наивный алгоритм

Будем говорить, что паттерн $p$ встречается в тексте $t$ со сдвигом $s$, если $0⩽s⩽n−m$ и $t[s..s+m−1]=p$. Если строка $p$ встречается в строке $t$, то $p$ является подстрокой $t$.

В наивном алгоритме поиск всех допустимых сдвигов производится с помощью цикла, в котором проверяется условие $t[s..s+m−1]=p$ для каждого из $n−m+1$ возможных значений $s$.

```py
def naiveStringMatcher(t, p):
    n = len(t)
    m = len(p)
    ans = []
    for i in range(0, n - m):
        if t[i : i + m - 1] == p:
            ans.push_back(i)
    return ans
```

---

# Наивный алгоритм. Время работы

Алгоритм работает за $O(m⋅(n−m))$. В худшем случае $m= { {n} \over {2} }$, что даёт $O({n^2 \over 4})=O(n^2)$. Однако если $m$ достаточно мало по сравнению с $n$, то тогда асимптотика получается близкой к $O(n)$, поэтому этот алгоритм достаточно широко применяется на практике.

---

# Наивный алгоритм. Сравнение с другими алгоритмами

## Преимущества

- Требует $O(1)$ памяти.
- Приемлемое время работы на практике. Благодаря этому алгоритм применяется, например, в браузерах и текстовых редакторах (при использовании Ctrl + F), потому что обычно паттерн, который нужно найти, очень короткий по сравнению с самим текстом. Также наивный алгоритм используется в стандартных библиотеках языков высокого уровня (C++, Java), потому что он не требует дополнительной памяти.
- Простая и понятная реализация.

## Недостатки

- Требует $O(m⋅(n−m))$ операций, вследствие чего алгоритм работает медленно в случае, когда длина паттерна достаточно велика.

---

# Z-функция

**Z-функция (Z-function)** от строки $S$ и позиции $x$ — это длина максимального префикса подстроки, начинающейся с позиции $x$ в строке $S$, который одновременно является и префиксом всей строки $S$. Более формально, $Z[i](s)=\max k∣s[i…i+k]=s[0…k]$.

Иными словами, $z[i]$ — это длина наибольшего общего префикса строки $s$ и её $i$-го суффикса.

Значение Z-функции от первой позиции не определено, поэтому его обычно приравнивают к нулю или к длине строки.

"aaaaa" - $[0, 4, 3, 2, 1]$

"aaabaab" - $[0, 2, 1, 0, 2, 1, 0]$

"abacaba" - $[0, 0, 1, 0, 3, 0, 1]$

---

# Z-функция. Тривиальный алгоритм

Формальное определение можно представить в виде следующей элементарной реализации за $O(n^2)$:

```python
def z_func(s, n):
    z = [0] * n
    for i in range(1, n):
        while i + z[i] < n and s[z[i]] == s[i + z[i]]:
            z[i] += 1
    return z
```

Мы просто для каждой позиции $i$ перебираем ответ для неё $z[i]$, начиная с нуля, и до тех пор, пока мы не обнаружим несовпадение или не дойдём до конца строки.

Разумеется, эта реализация слишком неэффективна, перейдём теперь к построению эффективного алгоритма.

---

# Z-функция. Эффективный алгоритм

Чтобы получить эффективный алгоритм, будем вычислять значения $z[i]$ по очереди — от $i=1$ до $n−1$, и при этом постараемся при вычислении очередного значения $z[i]$ максимально использовать уже вычисленные значения.

Назовём для краткости подстроку, совпадающую с префиксом строки $s$, отрезком совпадения. Например, значение искомой Z-функции $z[i]$ — это длина длиннейшего отрезка совпадения, начинающегося в позиции $i$ (и заканчиваться он будет в позиции $i+z[i]−1$).

Для этого будем поддерживать координаты $[l;r]$ самого правого отрезка совпадения, т.е. из всех обнаруженных отрезков будем хранить тот, который оканчивается правее всего. В некотором смысле, индекс $r$ — это такая граница, до которой наша строка уже была просканирована алгоритмом, а всё остальное — пока ещё не известно.

---

# Z-функция. Эффективный алгоритм

Тогда если текущий индекс, для которого мы хотим посчитать очередное значение $Z$-функции, — это $i$, мы имеем один из двух вариантов:

1. $i>r$ — т.е. текущая позиция лежит за пределами того, что мы уже успели обработать.
   Тогда будем искать $z[i]$ тривиальным алгоритмом, т.е. просто пробуя значения $z[i]=0$, $z[i]=1$, и т.д. Заметим, что в итоге, если $z[i]$ окажется $>0$, то мы будем обязаны обновить координаты самого правого отрезка $[l;r]$ — т.к. $i+z[i]−1$ гарантированно окажется больше $r$.

---

# Z-функция. Эффективный алгоритм

2. $i≤r$ — т.е. текущая позиция лежит внутри отрезка совпадения $[l;r]$.
   Тогда мы можем использовать уже подсчитанные предыдущие значения Z-функции, чтобы проинициализировать значение $z[i]$ не нулём, а каким-то возможно бОльшим числом.

В качестве начального приближения для $z[i]$ безопасно брать только такое выражение:

$$
z_0[i]=\min(r−i+1,z[i−l]).
$$

Проинициализировав $z[i]$ таким значением $z_0[i]$, мы снова дальше действуем тривиальным алгоритмом — потому что после границы $r$, вообще говоря, могло обнаружиться продолжение отрезка совпадения, предугадать которое одними лишь предыдущими значениями Z-функции мы не можем.

---

# Z-функция. Эффективный алгоритм

Таким образом, весь алгоритм представляет из себя два случая, которые фактически различаются только начальным значением $z[i]$: в первом случае оно полагается равным нулю, а во втором — определяется по предыдущим значениям по указанной формуле. После этого обе ветки алгоритма сводятся к выполнению тривиального алгоритма, стартующего сразу с указанного начального значения.

Алгоритм получился весьма простым. Несмотря на то, что при каждом $i$ в нём так или иначе выполняется тривиальный алгоритм — мы достигли существенного прогресса, получив алгоритм, работающий за линейное время.

---

# Z-функция. Эффективный алгоритм

```py
def z_func(s):
    n = len(s)
    z = [0] * n
    l = 0
    r = 0
    for i in range(1, n):
        if i < r:
            z[i] = min(r - i, z[i - l])
        while i + z[i] < n and s[z[i]] == s[i + z[i]]:
            z[i] += 1
        if i + z[i] > r:
            l = i
            r = i + z[i]
    return z
```

---

# Поиск подстроки в строке с помощью Z-функции

$n$ — длина текста. $m$ — длина образца.
Образуем строку `s = pattern + # + text`, где `#` — символ, не встречающийся ни в `text`, ни в `pattern`. Вычисляем Z-функцию от этой строки. В полученном массиве, в позициях в которых значение Z-функции равно `|pattern|`, по определению начинается подстрока, совпадающая с $pattern$.

```python
def substringSearch(text, pattern):
    zf = z_func(f"{pattern}#{text}")
    for i in range(m + 1, n + 1):
        if zf[i] == m
            return i
```

---

# Префикс функция

Пусть дана строка $s$ длины $n$. Тогда $π(s)$ - это массив длины $n$
, $i$-ый элемент которого ($π[i]$) определяется следующим образом: это длина наибольшего собственного суффикса подстроки $s[0…i]$, совпадающего с её префиксом (собственный суффикс — значит не совпадающий со всей строкой). В частности, значение $π[0]$ полагается равным нулю.

Например, для строки **"abcabcd"** префикс-функция равна: $[0,0,0,1,2,3,0]$, что означает:

    у строки "a" нет нетривиального префикса, совпадающего с суффиксом;
    у строки "ab" нет нетривиального префикса, совпадающего с суффиксом;
    у строки "abc" нет нетривиального префикса, совпадающего с суффиксом;
    у строки "abca" префикс длины 1 совпадает с суффиксом;
    у строки "abcab" префикс длины 2 совпадает с суффиксом;
    у строки "abcabc" префикс длины 3 совпадает с суффиксом;
    у строки "abcabcd" нет нетривиального префикса, совпадающего с суффиксом.

Другой пример — для строки **"aabaaab"** она равна: $[0,1,0,1,2,2,3]$.

---

# Префикс функция. Тривиальный алгоритм

```python
def prefix_func(s):
    n = len(s)
    pi = [0] * n
    for i in range(n - 1):
        for k in range(1, i + 1):
            for j in range(k):
                if s[j] != s[i - k  + 1 + j]:
                    break
            else:
                pi[i] = k
    return pi
```

Как нетрудно заметить, работать он будет за $O(n^3)$, что слишком медленно.

---

# Префикс функция. Эффективный алгоритм

Для удобства будем обозначать подстроки строки $s$ следующим образом: пусть $p^k$ - префикс $s$ длины $k$, $s^k_i$ - подстрока длины $k$ заканчивающаяся символом с номером $i$. Напомним, что первый символ строки имеет номер $0$.

Будем вычислять $π[i]$ последовательно, начиная с $π[1]$. $π[0]$ очевидно $=0$. Постараемся на $i$ шаге получить решение, используя уже известную информацию, т.е. предыдущие значения $π$.

---

# Префикс функция. Эффективный алгоритм

```python
def prefix_func(s):
    n = len(s)
    pi = [0] * n
    for i in range(1, n):
        j = pi[i - 1]
        while j > 0 and s[i] != s[j]:
            j = pi[j - 1]
        if s[i] == s[j]:
            j += 1
        pi[i] = j
    return pi
```

---

# Алгоритм Кнута-Морриса-Пратта

Дан текст $t$ и строка $s$, требуется найти и вывести позиции всех вхождений строки $s$ в текст $t$.

Обозначим для удобства через $n$ длину строки $s$, а через $m$ — длину текста $t$.

Образуем строку `s+#+t`, где символ `#` — это разделитель, который не должен нигде более встречаться. Посчитаем для этой строки префикс-функцию. Теперь рассмотрим её значения, кроме первых $n+1$ (которые, как видно, относятся к строке $s$ и разделителю). По определению, значение $π[i]$ показывает наидлиннейшую длину подстроки, оканчивающейся в позиции $i$ и совпадающего с префиксом. Но в нашем случае это $π[i]$ — фактически длина наибольшего блока совпадения со строкой $s$ и оканчивающегося в позиции $i$. Больше, чем $n$, эта длина быть не может, за счёт разделителя. А вот равенство $π[i]=n$ (там, где оно достигается), означает, что в позиции $i$ оканчивается искомое вхождение строки $s$ (только не надо забывать, что все позиции отсчитываются в склеенной строке `s+#+t`).


---

# Алгоритм Кнута-Морриса-Пратта

Таким образом, если в какой-то позиции $i$ оказалось $π[i]=n$, то в позиции $i−(n+1)−n+1=i−2n$ строки $t$ начинается очередное вхождение строки $s$ в строку $t$.

Как уже упоминалось при описании алгоритма вычисления префикс-функции, если известно, что значения префикс-функции не будут превышать некоторой величины, то достаточно хранить не всю строку и префикс-функцию, а только её начало. В нашем случае это означает, что нужно хранить в памяти лишь строку `s+#` и значение префикс-функции на ней, а потом уже считывать по одному символу строку $t$ и пересчитывать текущее значение префикс-функции.

Алгоритм Кнута-Морриса-Пратта решает эту задачу за $O(n+m)$ времени и $O(n)$ памяти.

---

# Алгоритм Кнута-Морриса-Пратта

```py
def kmp(s, t):
    n = len(s)
    m = len(t)
    answer = []
    p = prefix_func(s + "#" + t)
    count = 0
    for i in range(0, m - 1)
        if p[n + i + 1] == n:
            count += 1
            answer.append(i - n)
    return answer
```

---

# Хеширование

**Хеширование (hashing)** — класс методов поиска, идея которого состоит в вычислении хеш-кода, однозначно определяемого элементом с помощью хеш-функции, и использовании его, как основы для поиска (индексирование в памяти по хеш-коду выполняется за $O(1)$). 

В общем случае, однозначного соответствия между исходными данными и хеш-кодом нет в силу того, что количество значений хеш-функций меньше, чем вариантов исходных данных, поэтому существуют элементы, имеющие одинаковые хеш-коды — так называемые **коллизии**, но если два элемента имеют разный хеш-код, то они гарантированно различаются. Вероятность возникновения коллизий играет немаловажную роль в оценке качества хеш-функций. Для того чтобы коллизии не замедляли работу с таблицей существуют методы для борьбы с ними.

---

# Полиномиальный хеш

Пусть дана строка $s[0..n−1]$. Тогда полиномиальным хешем (polynomial hash) строки $s$ называется число $h=hash(s[0..n−1])=p^0s[0]+...+p^{n−1}s[n−1]$, где $p$ — некоторое простое число, а $s[i]$ − код $i$-ого символа строки $s$.

<v-clicks>

## Пример:

$$
s = \text{"sirius"}, p = 3
$$
$$
h = hash(s) = 3^0 \cdot 115 + 3^1 \cdot 105 + 3^2 \cdot 114 + 3^3 \cdot 105 + 3^4 \cdot 117 + 3^5 \cdot 115 = \\ = 115 + 315 + 1026 + 2835 + 9477 + 27945 = 41713
$$

</v-clicks>

---

# Поиск подстроки с помощью хэш-функции

Для работы алгоритма поиска подстроки потребуется считать хеш подстроки $s[i..j]$. Делать это можно следующим образом:

Рассмотрим хеш $s[0..j]$:

$$
hash(s[0..j])=s[0]+ps[1]+...+p^{i−1}s[i−1]+p^is[i]+...+p^{j−1}s[j−1]+p^js[j] \\
hash(s[0..j])=(s[0]+ps[1]+...+p^{i−1}s[i−1])+(p^is[i]+...+p^{j−1}s[j−1]+p^js[j])\\
hash(s[0..j])=(s[0]+ps[1]+...+p^{i−1}s[i−1])+\\+p^i(s[i]+...+p^{j−i−1}s[j−1]+p^{j−i}s[j])
$$

Выражение в первой скобке есть не что иное, как хеш подстроки $s[0..i−1]$, а во второй — хеш нужной нам подстроки $s[i..j]$.

---

# Поиск подстроки с помощью хэш-функции

Итак, мы получили, что:

$$hash(s[0..j])=hash(s[0..i−1])+p^ihash(s[i..j])$$

Отсюда получается следующая формула для $hash(s[i..j])$:

$$hash(s[i..j])=(1/p^i)(hash(s[0..j])−hash(s[0..i−1]))$$

---

# Поиск подстроки с помощью хэш-функции

Однако, как видно из формулы, чтобы уметь считать хеш для всех подстрок начинающихся с $i$, нужно предпосчитать все $p^i$ для $i∈[0..n−1]$. Это займет много памяти. Но поскольку нам нужны только подстроки размером $m$ − мы можем подсчитать хеш подстроки $s[0..m−1]$, а затем пересчитывать хеши для всех $i∈[0..n−m]$ за $O(1)$ следующим образом:

$$hash(s[i+1..i+m−1])=(hash(s[i..i+m−1])−p^{m−1}s[i])\mod r$$

$$hash(s[i+1..i+m])=(p⋅hash(s[i+1..i+m−1])+s[i+m]) \mod r$$

Получается: $hash(s[i+1..i+m])=(p⋅hash(s[i..i+m−1])−p^is[i]+s[i+m]) \mod r$

---

# Алгоритм Рабина-Карпа

Алгоритм начинается с подсчета $hash(s[0..m−1])$ и $hash(p[0..m−1])$, а также с подсчета $p^m$, для ускорения ответов на запрос.

Для $i∈[0..n−m]$ вычисляется $hash(s[i..i+m−1])$ и сравнивается с $hash(p[0..m−1])$. Если они оказались равны, то образец $p$ скорее всего содержится в строке $s$ начиная с позиции $i$, хотя возможны и ложные срабатывания алгоритма. Если требуется свести такие срабатывания к минимуму или исключить вовсе, то применяют сравнение некоторых символов из этих строк, которые выбраны случайным образом, или применяют явное сравнение строк, как в наивном алгоритме поиска подстроки в строке.

Если требуется найти индексы вхождения нескольких образцов, или сравнить две строки − выгоднее будет предпосчитать все степени $p$, а также хеши всех префиксов строки $s$.

---

# Алгоритм Рабина-Карпа

Алгоритм находит все вхождения строки $w$ в строку $s$ и возвращает массив позиций, откуда начинаются вхождения.

```python
def rabinKarp(s, w):
   answer = []
   n, m = len(s), len(w)
   hashS, hashW = hash(s[:m]), hash(w[:m])
   for i in range(n - m):
        if hashS == hashW:
            answer.add(i)
        hashS = (p * hashS - (p ** m) * hash(s[i]) + hash(s[i + m])) % r
   return answer
```

Новый хеш $hashS$ был получен с помощью быстрого пересчёта. Для сохранения корректности алгоритма нужно считать, что $s[n+1]$ — пустой символ.

---

# Хеш-таблицы

**Хеш-таблица (англ. hash-table)** — структура данных, реализующая интерфейс ассоциативного массива. В отличие от деревьев поиска, реализующих тот же интерфейс, обеспечивают меньшее время отклика в среднем. Представляет собой эффективную структуру данных для реализации словарей, а именно, она позволяет хранить пары (ключ, значение) и выполнять три операции: операцию добавления новой пары, операцию поиска и операцию удаления пары по ключу.

Существует два основных вида хеш-таблиц: с цепочками и открытой адресацией. Хеш-таблица содержит некоторый массив $H$, элементы которого есть пары (хеш-таблица с открытой адресацией) или списки пар (хеш-таблица с цепочками).

Выполнение операции в хеш-таблице начинается с вычисления хеш-функции от ключа. Хеш-код $i=h(key)$ играет роль индекса в массиве $H$, а зная индекс, мы можем выполнить требующуюся операцию (добавление, удаление или поиск).

---

# Коллизии

**Коллизия (collision)**: $∃x≠y:h(x)=h(y)$ - существует x не равный y, такой что h(x)=h(y).

**Разрешение коллизий (collision resolution)** в хеш-таблице, задача, решаемая несколькими способами: метод цепочек, открытая адресация и т.д. Очень важно сводить количество коллизий к минимуму, так как это увеличивает время работы с хеш-таблицами.

---

# Разрешение коллизий с помощью цепочек

Каждая ячейка $i$ массива $H$ содержит указатель на начало списка всех элементов, хеш-код которых равен $i$, либо указывает на их отсутствие. Коллизии приводят к тому, что появляются списки размером больше одного элемента.

В зависимости от того нужна ли нам уникальность значений операция вставки у нас будет работать за разное время. Если не важна, то мы используем список, время вставки в который будет в худшем случае равна $O(1)$. Иначе мы проверяем есть ли в списке данный элемент, а потом в случае его отсутствия мы его добавляем. В таком случае вставка элемента в худшем случае будет выполнена за $O(n)$

---

# Разрешение коллизий с помощью цепочек

Время работы поиска в наихудшем случае пропорционально длине списка, а если все $n$ ключей захешировались в одну и ту же ячейку (создав список длиной $n$) время поиска будет равно $Θ(n)$ плюс время вычисления хеш-функции, что ничуть не лучше, чем использование связного списка для хранения всех $n$ элементов.

Удаления элемента может быть выполнено за $O(1)$, как и вставка, при использовании двухсвязного списка.

---

# Разрешение коллизий с помощью цепочек

::center

![Alt text ](/img/9/image-13.png) {width=600px lazy}

::

---

# Открытая адресация

Все элементы хранятся непосредственно в хеш-таблице, без использования связных списков. В отличие от хеширования с цепочками, при использовании этого метода может возникнуть ситуация, когда хеш-таблица окажется полностью заполненной, следовательно, будет невозможно добавлять в неё новые элементы. Так что при возникновении такой ситуации решением может быть динамическое увеличение размера хеш-таблицы, с одновременной её перестройкой.

---

# Открытая адресация

**Последовательный поиск**
При попытке добавить элемент в занятую ячейку $i$ начинаем последовательно просматривать ячейки $i+1,i+2,i+3$ и так далее, пока не найдём свободную ячейку. В неё и запишем элемент.

::center

![Alt text height:350](/img/9/image-15.png) {width=500px}

::

---

# Открытая адресация

**Линейный поиск**
Выбираем шаг $q$. При попытке добавить элемент в занятую ячейку $i$ начинаем последовательно просматривать ячейки $i+(1⋅q),i+(2⋅q),i+(3⋅q)$ и так далее, пока не найдём свободную ячейку. В неё и запишем элемент. По сути последовательный поиск - частный случай линейного, где $q=1$.

::center

![Alt text height:300](/img/9/image-16.png) {width=380px}

::

---

# Открытая адресация

**Квадратичный поиск**
Шаг $q$ не фиксирован, а изменяется квадратично: $q=1,4,9,16...$. Соответственно при попытке добавить элемент в занятую ячейку $i$ начинаем последовательно просматривать ячейки $i+1,i+4,i+9$ и так далее, пока не найдём свободную ячейку.

::center

![Alt text height:300](/img/9/image-17.png) {width=450px}

::

---

# Гипотеза равномерного хеширования

**Универсальное хеширование (Universal hashing)** — это вид хеширования, при котором используется не одна конкретная хеш-функция, а происходит выбор из заданного семейства по случайному алгоритму. Такой подход обеспечивает равномерное хеширование: для очередного ключа вероятности помещения его в любую ячейку совпадают. Известно несколько семейств универсальных хеш-функций, которые имеют многочисленные применения в информатике, в частности в хеш-таблицах, вероятностных алгоритмах и криптографии.

Впервые понятие универсального хеширования было введено в статье Картера и Вегмана в 1979 году.

---

# Гипотеза равномерного хеширования

Созданный алгоритм универсального хеширования представлял собой случайный выбор хеш-функции из некоторого набора хеш-функций(называемого универсальным семейством хеш-функций), обладающих определёнными свойствами. Авторами было показано, что в случае универсального хеширования число обращений к хеш-таблице (в среднем по всем функциям из семейства) для произвольных входных данных оказывается очень близким теоретическому минимуму для случая фиксированной хеш-функции со случайно распределёнными входными данными.

---

# Гипотеза равномерного хеширования

Семейство хеш-функций $H$ называется **универсальным**, если

$$\forall x,y\in U\longrightarrow \delta _{H}(x,S)={\frac {\left|H\right|}{m}}$$

Универсальные семейства хеш-функций для:

1. **Чисел**

   $p$ - некоторое простое число
   $$h_{a,b}(x)=((ax+b)\mod p)\mod m$$

---

# Гипотеза равномерного хеширования

2. **Векторов**

   Пусть число $m$ является простым. Пусть входные данные $x$ представлены как последовательность $r+1$ элементов, принадлежащих $\left\{0,1,..,p-1\right\}$, то есть $x=\left\langle x_{0},x_{1},...,x_{r}\right\rangle$.

   Для всех последовательностей вида ${\displaystyle a=\left\langle a_{0},a_{1},...,a_{r}\right\rangle ,a_{i}\in \mathbb {Z} _{p},i={\overline {0,r}}}$ рассмотрим функцию $h_{a}$ вида
   $${\displaystyle h_{a}(x)=\sum _{i=0}^{r}{a_{i}x_{i}}\mod m}$$

---

# Гипотеза равномерного хеширования

3. **Строк**
   $$h_{a}({\bar {x}})=h_{a}^{\mathrm {int} }\left({\big (}\sum _{i=0}^{\ell }x_{i}\cdot a^{i}{\big )}{\bmod {~}}p\right),$$
   где $h_{a}^{\mathrm {int} }:\left\{0,1,..,p-1\right\}\rightarrow \left\{0,1,..,m-1\right\}$ является универсальной хеш-функцией для числовых аргументов.
